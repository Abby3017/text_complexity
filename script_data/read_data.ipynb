{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import tarfile\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading corpus data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to read data file using pandas from github repo\n",
    "url= 'https://github.com/scrosseye/CLEAR-Corpus/blob/main/CLEAR_corpus_final.xlsx?raw=True'\n",
    "myfile = requests.get(url)\n",
    "\n",
    "df=pd.read_excel(url, sheet_name='Data', engine='openpyxl')\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus of Sentences rated with Human Complexity Judgments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './../data/complexity_ds_en.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REALEC Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open tar.gz file name Exam2016.tar.gz in data folder and print all files in that that have .txt and .json extension, then store them in a list\n",
    "# all content of txt file as string and from json file read the content of key 'ielts', 'text' and store them as pandas dataframe\n",
    "\n",
    "file_path = './../data/Exam2016.tar.gz'\n",
    "count = 0\n",
    "dict_text = {}\n",
    "dict_json = {}\n",
    "data = []\n",
    "with tarfile.open(file_path, \"r:gz\") as tar:\n",
    "    count += 1\n",
    "    for tarinfo in tar:\n",
    "        if tarinfo.name.endswith(\".txt\") or tarinfo.name.endswith(\".json\"):\n",
    "            file_name = tarinfo.name\n",
    "            file_name = file_name.split('/')[1].split('.')[0]\n",
    "            with tar.extractfile(tarinfo) as f:\n",
    "                if tarinfo.name.endswith(\".txt\"):\n",
    "                    str_sentence = f.read()\n",
    "                    dict_text[file_name] = str_sentence\n",
    "                else:\n",
    "                    json_data = pd.read_json(f, orient='index').T\n",
    "                    dict_json[file_name] = json_data\n",
    "\n",
    "\n",
    "for key in dict_text.keys():\n",
    "    datum = []\n",
    "    sentence = dict_text[key]\n",
    "    json_data = dict_json[key]\n",
    "    datum = [sentence, json_data['ielts'][0], json_data['CEFR_level'][0], json_data['work_type'][0], json_data['year'][0]]\n",
    "    data.append(datum)\n",
    "\n",
    "df_pd = pd.DataFrame(data, columns=['sentence', 'ielts', 'CEFR_level', 'work_type', 'year'])\n",
    "df_pd = df_pd[df_pd['CEFR_level'] != '']\n",
    "df_pd.to_csv('./../data/Exam2016.csv', index=False, header=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MMLL - EFCAMDAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efcamdat_file_path = './../data/Final database (main prompts).xlsx'\n",
    "df = pd.read_excel(efcamdat_file_path, sheet_name='Sheet 1')\n",
    "efcamdat_df = df[['writing_id', 'cefr', 'cefr_numeric', 'level', 'grade', 'wordcount', 'mtld', 'text', 'text_corrected']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efcamdat_file_path = './../data/Final database (alternative prompts).xlsx'\n",
    "df = pd.read_excel(efcamdat_file_path, sheet_name='Sheet 1')\n",
    "efcamdat_df = df[['writing_id', 'cefr', 'cefr_numeric', 'level', 'grade', 'wordcount', 'mtld', 'text', 'text_corrected']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEWSELA Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://s3.amazonaws.com/newsela-research-corpora/newsela_article_corpus_with_scripts_2016-01-29.1.zip\"\n",
    "# read zip file from URL and also read the given csv file in the zip file\n",
    "response = requests.get(url)\n",
    "df = pd.DataFrame()\n",
    "with zipfile.ZipFile(BytesIO(response.content)) as z:\n",
    "    with z.open('newsela_article_corpus_2016-01-29/articles_metadata.csv') as f:\n",
    "        df = pd.read_csv(f)\n",
    "        print(df.info())\n",
    "    df['content'] = df['filename'].apply(lambda x: z.open('newsela_article_corpus_2016-01-29/articles/' + x).read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASSET Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asset.valid.orig\n",
      "asset.valid.simp.0\n",
      "asset.valid.simp.1\n",
      "asset.valid.simp.2\n",
      "asset.valid.simp.3\n",
      "asset.valid.simp.4\n",
      "asset.valid.simp.5\n",
      "asset.valid.simp.6\n",
      "asset.valid.simp.7\n",
      "asset.valid.simp.8\n",
      "asset.valid.simp.9\n"
     ]
    }
   ],
   "source": [
    "username = \"facebookresearch\"\n",
    "repo_name = \"asset\"\n",
    "folder_path = \"dataset\"\n",
    "url = f\"https://api.github.com/repos/{username}/{repo_name}/contents/{folder_path}\"\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "files = response.json()\n",
    "file_series = []\n",
    "for file in files:\n",
    "    if file['type'] == 'file' and 'valid' in file['name']:\n",
    "            print(file['name'])\n",
    "            file_contents = []\n",
    "            content_url = file['download_url']\n",
    "            content_response = requests.get(content_url)\n",
    "            content_response.raise_for_status()\n",
    "            content = content_response.text\n",
    "            file_contents.extend(content.splitlines())\n",
    "            series = pd.Series(file_contents)\n",
    "            file_series.append(series)\n",
    "\n",
    "df = pd.concat(file_series, axis=1)\n",
    "df.columns =['Original', 'Simplified1', 'Simplified2', 'Simplified3', 'Simplified4', 'Simplified5', 'Simplified6', 'Simplified7',\n",
    "'Simplified8', 'Simplified9', 'Simplified10']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
